{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+liAdlXbTe0fUQEHbKiZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredyah/mytorchscript/blob/main/4_Create_Model_Class_for_Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "-VfOdYbhMSVi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Class\n",
        "## fc1 的 in_features 值可參考 3_convolutional_Neural_Network.ipynb 最後的結果 torch.Size([1, 16, 5, 5])\n",
        "\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1)\n",
        "\n",
        "    ## Fully Connected Layer\n",
        "    self.fc1 = nn.Linear(in_features=5*5*16, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    ## First Pass\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(input=X, kernel_size=2, stride=2)\n",
        "\n",
        "    ## Second Pass\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(input=X, kernel_size=2, stride=2)\n",
        "\n",
        "    ## Re-View to flatten it out\n",
        "    X = X.view(-1, 16*5*5)  ## 調整 X shape ，使其符合 full connected in_features )\n",
        "\n",
        "    ## Fully Connected Layers\n",
        "    ## relu 是一種常用激活函數，全稱為修正線性單元。它的主要作用是將輸入值限制在一個非負的範圍內，即常輸入值小於0時，輸出值為0，當輸入值大於等於0時，輸出值等於輸入值本身。relu函數的表達式為：f(x)=max(0, x)\n",
        "    ## 請注意，最後一個fc ，通常為輸出，不需要用的 relu\n",
        "\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "EvPM1hlHMtj3"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert MNIST Image Files into a Tensor of 4-Dimensions ( of images, Height, Width, Color Channel )\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "## Train Data\n",
        "\n",
        "train_data = datasets.MNIST('/cnn_data', train=True, download=True, transform=transform)\n",
        "\n",
        "## Test Data\n",
        "\n",
        "test_data = datasets.MNIST('/cnn_data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "## Create a small batch size for image\n",
        "##  Batch size為批次的尺寸或稱作批次的大小，表示我們在讓機器學習資料集的知識時，一次是對多少筆資料做觀察整理的，然後對於這些資料的結論去做資料調整\n",
        "'''\n",
        "在使用 PyTorch 構建和訓練卷積神經網絡（CNN）時，shuffle 參數主要出現在數據加載器（DataLoader）中。它的作用是指示是否在每個 epoch 開始時將數據集的順序打亂。這對於提升模型的泛化能力非常重要。\n",
        "\n",
        "為什麼需要 Shuffle？\n",
        "打破數據集的順序依賴性：在許多數據集中，數據的排列順序可能具有某種模式或結構。如果模型總是按照固定的順序看到數據，可能會導致學習到無用的模式或過擬合特定的數據順序。\n",
        "提高泛化能力：隨機打亂數據有助於模型更全面地學習數據分佈，從而提高對未見數據的泛化能力。\n",
        "避免局部依賴性：通過打亂數據順序，可以確保每個小批量（batch）中的數據樣本是隨機的，減少局部依賴性，進一步提高訓練的穩定性。\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yfOFx2i91V8W"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create an Instance of our Model\n",
        "\n",
        "torch.manual_seed(41) ## 讓結果可複現\n",
        "\n",
        "model = ConvolutionalNetwork()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbWM3WFOMzdc",
        "outputId": "ba77ffe9-a675-406f-8587-70d3163b64be"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvolutionalNetwork(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss Function Optimizer\n",
        "'''\n",
        "Cross Entropy Loss：\n",
        "-------------------------\n",
        "交叉熵損失（Cross-Entropy Loss）：這是一種常用的損失函數，特別適用於多類分類問題。它計算預測值與真實標籤之間的差異。具體來說，它衡量了模型輸出的概率分佈與真實標籤分佈之間的距離。\n",
        "PyTorch 中的 nn.CrossEntropyLoss：這個函數自動將輸入的 logits 進行 softmax 變換，並計算最終的交叉熵損失。這意味著在使用這個損失函數時，你的模型輸出不需要顯式地應用 softmax 函數。\n",
        "\n",
        "\n",
        "Adam：\n",
        "-----------------\n",
        "Adam 優化器：Adam（Adaptive Moment Estimation）是一種基於一階和二階矩估計自適應學習率的優化算法。它結合了 AdaGrad 和 RMSProp 的優點，在處理稀疏梯度和高噪聲問題上表現良好。Adam 是目前深度學習領域中最常用的優化算法之一。\n",
        "\n",
        "lr\n",
        "-----------------\n",
        "這是學習率（Learning Rate），決定了每次更新參數時步伐的大小。較小的學習率會使訓練過程更穩定，但也會使訓練時間變長。選擇合適的學習率是模型訓練中的關鍵步驟。\n",
        "常見的學習率範圍是 0.001 到 0.1。\n",
        "\n",
        "'''\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()    ## 定義使用的標準\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)    ## 指定最佳化器 Smaller the Learning Rate, longer its gonna take to train.\n"
      ],
      "metadata": {
        "id": "1Zk_A8YAY1Qs"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "## Create Variables To Tracks Things\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "## For Loop of Epochs\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "\n",
        "  ## Train\n",
        "  for b, (X_train, y_train) in enumerate(train_loader):\n",
        "    b+=1  ## start our batches at 1\n",
        "    y_pred = model(X_train)   ## get predicted values from the training set, Not flattened 2D\n",
        "    loss = criterion(y_pred, y_train)  ## how off are we ? Compare the predictions to correct answers in y_train\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1]  ## add up the number of correct predictions. Indexed off the first point\n",
        "    batch_corr = (predicted == y_train).sum()   ## how many we got correct from this batch. True = 1, False = 0, sum those up\n",
        "    trn_corr += batch_corr  ## Keep track as we go along in training\n",
        "    '''\n",
        "    torch.max 用于返回给定张量中的最大值及其索引。具体来说：\n",
        "    y_pred 是你的模型输出的预测结果张量。\n",
        "    .data 获取张量的原始数据。\n",
        "    torch.max(y_pred.data, 1) 将沿着指定维度（这里是第 1 维，即列）返回每行的最大值和索引。\n",
        "    [1] 获取最大值所在的索引。\n",
        "    通过这个代码，你可以得到模型预测的类别索引。\n",
        "    '''\n",
        "\n",
        "    ## Update our parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b%600 == 0:\n",
        "      print(f\"Epoch: {i} Batch: {b} Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "  ## Test\n",
        "  with torch.no_grad():  ## No gradient so we don't update our weights and biases with test data\n",
        "    for b, (X_test, y_test) in enumerate(test_loader):\n",
        "      y_val = model(X_test)\n",
        "      predicted = torch.max(y_val.data, 1)[1]   ## Adding up correct predictions\n",
        "      tst_corr += (predicted == y_test).sum()    ## T=1 F=0 and sum away\n",
        "\n",
        "  loss = criterion(y_val, y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_corr)\n",
        "\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f\"Training Took: {total/60} minutes !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D09jZjRbFDf",
        "outputId": "70c9e7ae-e801-4b3a-97ff-88d3658a7c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Batch: 600 Loss: 0.16236238181591034\n",
            "Epoch: 0 Batch: 1200 Loss: 0.16415396332740784\n",
            "Epoch: 0 Batch: 1800 Loss: 0.5098981261253357\n",
            "Epoch: 0 Batch: 2400 Loss: 0.11590572446584702\n",
            "Epoch: 0 Batch: 3000 Loss: 0.005982336588203907\n",
            "Epoch: 0 Batch: 3600 Loss: 0.5348214507102966\n",
            "Epoch: 0 Batch: 4200 Loss: 0.005393340718001127\n",
            "Epoch: 0 Batch: 4800 Loss: 0.0014583868905901909\n",
            "Epoch: 0 Batch: 5400 Loss: 0.03396736457943916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Graph the loss at epoch\n",
        "train_losses = [tl.item() for tl in train_losses]\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Validation Loss')\n",
        "plt.title('Loss at Epoch')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "FgiJHmBI_ESx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Graph the accuracy at the end of each epoch\n",
        "\n",
        "plt.plot([t/600 for t in train_correct], label='Training Accuracy')\n",
        "plt.plot([t/100 for t in test_correct], label='Validation Accuracy')\n",
        "plt.title('Accuracy at the end of each Epoch')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "IyQibiPyAUtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_load_everything = DataLoader(test_data, batch_size=10000, shuffle=False)\n"
      ],
      "metadata": {
        "id": "f7anQoNPBJB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  for X_test, y_test in test_load_everything:\n",
        "    y_val = model(X_test)\n",
        "    predicted = torch.max(y_val, 1)[1]\n",
        "    correct += (predicted == y_test).sum()\n"
      ],
      "metadata": {
        "id": "D8qw53rWBzMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Correct Rate: {correct.item()/len(test_data)*100}%\")"
      ],
      "metadata": {
        "id": "I3ojm8MFCbHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Grab just the data\n",
        "image_list = [4143, 4148, 4433, 4455, 1978, 1987, 1900, 2024]\n",
        "for il in image_list:\n",
        "  plt.imshow(test_data[il][0].reshape(28, 28))"
      ],
      "metadata": {
        "id": "c6Tqo_dODQRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pass the image thru our model\n",
        "\n",
        "for il in image_list:\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    new_prediction = model(test_data[il][0].view(1, 1, 28, 28))   ## batch size of 1, 1 color channel, 28*28 image\n",
        "\n",
        "    ## Check the new prediction , get probailities\n",
        "    print(new_prediction)\n",
        "    print(new_prediction.argmax())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qc66dDmLEDgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}